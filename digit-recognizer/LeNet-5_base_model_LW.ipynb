{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ffbd17-9bfa-4d49-907a-c62c2e0ac935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 base model work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f2cdf2-cfa3-4557-bce9-311a4aa5b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with the parameters from the original 1998 paper\n",
    "# will then work on iterative optimization of different areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21665c2b-173a-486d-9eb7-fa54df51e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.17.2-cp312-cp312-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.2.2-cp312-cp312-macosx_10_13_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.2.2 torchaudio-2.2.2 torchvision-0.17.2\n"
     ]
    }
   ],
   "source": [
    "# install torch\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f723308e-cbec-418d-8c1b-0384b0855e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfceffb-d6f7-4b69-a54a-dfb80c881b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd68ea4e-b119-48da-81ff-4db92de8a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load MNIST with only ToTensor() (no normalization)\n",
    "raw_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fcf923-b62f-4aa0-b426-34c9da33b127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lilywenger/MLM25/GitProjects/DigitRecognizer_Inkvestigators_mlm25/digit-recognizer'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da1743b-f444-4e68-a9f0-7b0ea3650086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8596538b-e64d-433c-a5f5-7faf6f635f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to read in the kaggle csv file, ensuring that the notebook doesn't use the pytorch dataset (which is different)\n",
    "\n",
    "# Load Kaggle train.csv\n",
    "train_df = pd.read_csv('train.csv')  # update path if needed\n",
    "\n",
    "# Separate features and labels \n",
    "X_train = train_df.drop('label', axis=1).values   # shape: (42000, 784)\n",
    "y_train = train_df['label'].values                # shape: (42000,)\n",
    "\n",
    "# Convert to PyTorch tensors and reshape \n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).view(-1, 1, 28, 28) / 255.0\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dadd6da5-9a90-4fc3-863c-7812c0fd893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in TensorDataset and DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1b9f1be-64e4-4ba9-9920-f6f7a740b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.1310, Std: 0.3085\n"
     ]
    }
   ],
   "source": [
    "# going to normalize the data, need the mean and std for z score normaliztion\n",
    "# only on the test set so that there is no test set data leakage\n",
    "mean = X_train_tensor.mean().item()\n",
    "std = X_train_tensor.std().item()\n",
    "print(f\"Mean: {mean:.4f}, Std: {std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c82a9-1e57-4de9-9c57-98f9b26f45b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16607eef-78f6-4e6c-b71d-f8109a7bf7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccfbd79f-63da-46e5-b6e0-dea0b5addcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b096974d-ddc8-4538-95da-9cf205d39d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all of the commented out commands below, these are using tensor transform\n",
    "# this only applies to one sample, [0], and i want to apply it to everything\n",
    "# i am going to do this manually now below the commented out information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "994fdeba-2524-4b64-a666-21ae827c154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z score normalization with calculated mean and std of the normalized (0 to 1) scores\n",
    "# defines what the transformation is\n",
    "#transform = transforms.Normalize((mean,), (std,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf16ac4-c80d-45ee-82a9-8745fe67f74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9f0c3-48fa-480d-b6a8-4bff06ae38e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5d11a1f-4da9-4e8c-8612-644ce49fdfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to a single batch for testing:\n",
    "#sample = X_train_tensor[0]  # shape [1,28,28]\n",
    "#normalized_sample = transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84467b2e-9d34-4960-af8d-0371b136717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the mean and std\n",
    "# should be 0 and 1\n",
    "#print(f\"Sample mean after normalization: {normalized_sample.mean():.4f}\")\n",
    "#print(f\"Sample std after normalization:  {normalized_sample.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "997be382-74c3-4235-a9e7-9cc24d7264ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply z-score normalization to the entire training set\n",
    "X_train_normalized = (X_train_tensor - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "688bd3f8-5693-46bb-b79e-1bfcbd285661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the normalized data in a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_normalized, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18c17ba5-bbee-48bd-badb-3d444efea119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean after normalization: 0.0000\n",
      "Std after normalization:  1.0000\n"
     ]
    }
   ],
   "source": [
    "# Check that normalization worked across the whole dataset\n",
    "print(f\"Mean after normalization: {X_train_normalized.mean():.4f}\")\n",
    "print(f\"Std after normalization:  {X_train_normalized.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f6845b9-4fb1-42bf-b5e8-1b4330adb29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this also created the train_loader data loader to use to feed into the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2fae4b4-3307-40b6-80ff-986ea1e688ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LeNet-5 Model\n",
    "# -------------------------------\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # First convolutional layer: 1 input channel (grayscale), 6 output channels, 5x5 kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        \n",
    "        # Second convolutional layer: 6 input channels, 16 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # flatten 16*4*4 feature maps into 120\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)           # 10 output classes for digits 0-9\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = torch.tanh(self.conv1(x))          # apply first conv layer + tanh activation\n",
    "        x = F.avg_pool2d(x, 2)                 # average pooling with 2x2 kernel\n",
    "        x = torch.tanh(self.conv2(x))          # second conv layer + tanh\n",
    "        x = F.avg_pool2d(x, 2)                 # second average pooling\n",
    "        x = x.view(-1, 16 * 4 * 4)             # flatten tensor for fully connected layers\n",
    "        x = torch.tanh(self.fc1(x))            # first FC layer + tanh\n",
    "        x = torch.tanh(self.fc2(x))            # second FC layer + tanh\n",
    "        x = self.fc3(x)                        # output layer (logits)\n",
    "        return F.log_softmax(x, dim=1)         # log softmax for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b7d35166-db2c-4734-9be6-a3090cdc5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Training\n",
    "# -------------------------------\n",
    "# Choose device: GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model and move it to the chosen device\n",
    "model = LeNet5().to(device)\n",
    "\n",
    "# Define the optimizer: SGD with learning rate 0.01 and momentum 0.9\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Define the loss function: CrossEntropyLoss is standard for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3622681a-71cd-450d-90c9-eac7904fb7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 done.\n",
      "Epoch 2/10 done.\n",
      "Epoch 3/10 done.\n",
      "Epoch 4/10 done.\n",
      "Epoch 5/10 done.\n",
      "Epoch 6/10 done.\n",
      "Epoch 7/10 done.\n",
      "Epoch 8/10 done.\n",
      "Epoch 9/10 done.\n",
      "Epoch 10/10 done.\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "# -------------------------------\n",
    "epochs = 10  # number of training epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # set model to training mode\n",
    "    for data, target in train_loader:  # iterate over batches\n",
    "        data, target = data.to(device), target.to(device)  # move batch to GPU/CPU\n",
    "        \n",
    "        optimizer.zero_grad()       # reset gradients from previous step\n",
    "        output = model(data)       # forward pass\n",
    "        loss = criterion(output, target)  # compute loss\n",
    "        loss.backward()            # backpropagation\n",
    "        optimizer.step()           # update model parameters\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} done.\")  # progress message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18bee0-dd3c-4434-b9f3-9dfd287ac722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0bcc82e-855d-413c-b28a-0c49c28d406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the test dataset\n",
    "test_df = pd.read_csv('test.csv')   \n",
    "X_test = test_df.values\n",
    "# For Kaggle, test.csv has no labels. \n",
    "# If you have validation labels, replace this line with y_test from validation CSV\n",
    "y_test = torch.zeros(X_test.shape[0], dtype=torch.long)  # placeholder if no labels\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).view(-1, 1, 28, 28) / 255.0\n",
    "y_test_tensor = y_test  # if labels available\n",
    "\n",
    "# Apply Z-score normalization to all images\n",
    "X_test_normalized = (X_test_tensor - mean) / std  # use training mean/std\n",
    "\n",
    "# load into test dataset loader\n",
    "# Wrap the normalized test data and labels into a TensorDataset\n",
    "test_dataset = TensorDataset(X_test_normalized, y_test_tensor)\n",
    "# DataLoader for the test set; no shuffling needed\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfc0d19d-6c81-49cf-8996-fd4fabef1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()  # evaluation mode\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():  # no gradients needed\n",
    "    for data, _ in test_loader:  # labels aren’t in test.csv\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)  # get predicted class\n",
    "        test_predictions.extend(pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7aed29bd-30b5-41f2-93a7-cb96da27a90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet-5_baseline_fulltrain_submission_LW_101925.csv ready for Kaggle!\n"
     ]
    }
   ],
   "source": [
    "# generating a submission file to submit to kaggle for accuracy predictions\n",
    "# i did not make an 80/20 split on the training data in the set, so i am just applying it to the test data\n",
    "submission = pd.DataFrame({\n",
    "    \"ImageId\": range(1, len(test_predictions)+1),\n",
    "    \"Label\": test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(\"LeNet-5_baseline_fulltrain_submission_LW_101925.csv\", index=False)\n",
    "print(\"LeNet-5_baseline_fulltrain_submission_LW_101925.csv ready for Kaggle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "712ae8d0-2e9a-4071-9c09-09f9be54ed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LeNet-5_base_model_LW.ipynb to script\n",
      "[NbConvertApp] Writing 7769 bytes to LeNet-5_base_model_LW.py\n"
     ]
    }
   ],
   "source": [
    "# convert to .py file\n",
    "!jupyter nbconvert --to script LeNet-5_base_model_LW.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37c82e-8ab9-4bee-9f41-25c2fb16df8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
